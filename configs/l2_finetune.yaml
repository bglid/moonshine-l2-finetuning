# Adapted from: https://github.com/pierre-cheneau/finetune-moonshine-asr/blob/main/configs/example_config.yaml
# Moonshine Fine-Tuning Configuration Example
# This file shows all available options with explanations
# =============================================================================
# DATASET CONFIGURATION
# =============================================================================
---
dataset:
  # HuggingFace dataset name or path to local dataset
  # name: "KoelLabs/L2Arctic"
  type: l2arctic

  # Language code (for multilingual datasets)
  language: "english"

  # Dataset splits to use
  train_percent: 0.9

  # Column names in your dataset
  audio_column: "audio" # Column containing audio files
  text_column: "text" # Column containing transcriptions

  # Optional: Subset of dataset (useful for testing)
  max_train_samples: null # null = use all, or specify a number (e.g., 1000)
  max_eval_samples: null

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
model:
  # Base model to fine-tune
  name: "UsefulSensors/moonshine-tiny"

  # Optional: Cache directory for downloading models
  cache_dir: null

  # Optional: Resume from checkpoint
  resume_from_checkpoint: null # e.g., "./results/checkpoint-1000"

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================
training:
  # Output directory for checkpoints and logs
  output_dir: "./results-moonshine-l2arctic"

  # Number of training epochs
  num_train_epochs: 2

  # Batch sizes (adjust based on your GPU memory)
  per_device_train_batch_size: 16 # 16 for 16GB GPU, 8 for 8GB GPU
  per_device_eval_batch_size: 16

  # Gradient accumulation (effective batch size = batch_size * accumulation_steps)
  gradient_accumulation_steps: 1

  # Learning rate
  learning_rate: 2e-5

  # Learning rate warmup
  warmup_steps: 500

  # Logging frequency
  logging_steps: 50
  logging_dir: "runs"

  # Evaluation frequency
  evaluation_strategy: "steps" # or "epoch"
  eval_steps: 250

  # Checkpoint saving
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 3 # Keep only last 3 checkpoints

  # Load best model at end
  load_best_model_at_end: true
  metric_for_best_model: "wer"
  greater_is_better: false

  # Mixed precision training (FP16)
  fp16: true

  # DataLoader settings
  dataloader_num_workers: 4
  dataloader_pin_memory: true

  # Gradient clipping
  max_grad_norm: 1.0

  # Random seed for reproducibility
  seed: 42

# =============================================================================
# OPTIMIZER CONFIGURATION
# =============================================================================
optimizer:
  # Optimizer type: "schedulefree_adamw" or "adamw"
  type: "schedulefree_adamw"

  # Adam beta parameters
  betas: [0.9, 0.999]

  # Adam epsilon
  epsilon: 1e-8

  # Weight decay for regularization
  weight_decay: 0.01

# =============================================================================
# DATA PROCESSING
# =============================================================================
data:
  # Audio duration limits (in seconds)
  max_duration: 15.0
  min_duration: 0.5

  # Preprocessing
  preprocessing_num_workers: 4

  # Audio normalization
  normalize_audio: true
  target_rms: 0.075

# =============================================================================
# GENERATION CONFIGURATION
# =============================================================================
generation:
  # Beam search parameters
  num_beams: 8
  max_new_tokens: 150

  # Repetition penalty
  repetition_penalty: 1.3
  no_repeat_ngram_size: 2

  # Early stopping
  early_stopping: true

# =============================================================================
# CURRICULUM LEARNING (Optional)
# =============================================================================
curriculum:
  # Enable curriculum learning (progressive difficulty)
  enabled: false

  # Training stages
  stages:
    # Stage 1: Short audio clips (easy)
    - duration: 2000 # steps
      max_audio_length: 5.0
      description: "Stage 1: Short clips (0.5-5s)"

    # Stage 2: Medium audio clips
    - duration: 3000
      max_audio_length: 10.0
      description: "Stage 2: Medium clips (0.5-10s)"

    # Stage 3: Full-length audio clips (hard)
    - duration: 3000
      max_audio_length: 20.0
      description: "Stage 3: Full length (0.5-20s)"

# =============================================================================
# DISTRIBUTED TRAINING (Optional)
# =============================================================================
distributed:
  # Enable distributed data parallel (DDP)
  enabled: false

  # Number of GPUs to use
  num_gpus: 1

  # Backend for DDP
  backend: "nccl"

# =============================================================================
# EVALUATION CONFIGURATION
# =============================================================================
evaluation:
  # Compute WER/CER during evaluation
  compute_metrics: true

  # Save predictions during evaluation
  save_predictions: true

  # Evaluation batch size
  batch_size: 16

# =============================================================================
# CALLBACKS (Optional)
# =============================================================================
callbacks:
  # Early stopping
  early_stopping:
    enabled: false
    patience: 5
    threshold: 0.01

  # TensorBoard logging
  tensorboard:
    enabled: true
# =============================================================================
# NOTES
# =============================================================================
# - Adjust batch_size based on your GPU memory:
#   * 8GB GPU: batch_size=8
#   * 16GB GPU: batch_size=16
#   * 24GB+ GPU: batch_size=32+
#
# - Use gradient_accumulation_steps if you want larger effective batch size
#   but have limited GPU memory
#
# - Enable curriculum learning for better convergence, especially with
#   diverse audio lengths
#
# - Schedule-free AdamW optimizer removes the need for learning rate schedules
#
# - Save checkpoints frequently at first, then reduce frequency once stable
